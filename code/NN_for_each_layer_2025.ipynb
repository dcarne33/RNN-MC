{"cells":[{"cell_type":"markdown","source":["# NN Monte Carlo 2\n","Author: Daniel Carne\n","\n","Date: 1/1/2025\n","\n","Description: Single NN for each number of layers for comparison in RNN paper"],"metadata":{"id":"OPHM4_fkus2_"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time"],"metadata":{"id":"D3YPtcOy_nIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9g8sJfCBSb7R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747924348885,"user_tz":240,"elapsed":16291,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"4aa02469-d837-4806-c8aa-470cb1c7008e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Load in data and seperate inputs and answers"],"metadata":{"id":"ot67Zycx6XWE"}},{"cell_type":"code","source":["dataset_a = np.loadtxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/D1.2_compileda.txt')\n","dataset_b = np.loadtxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/D1.2_compiledb.txt')\n","dataset = np.concatenate((dataset_a, dataset_b), axis=0)\n","print(dataset.shape)\n","input_t = np.zeros((len(dataset[:, 0]), 4))\n","input_t[:, :] = dataset[:, :4]\n","# dimensionless mu_a*t and mu_s*t\n","input_t[:, 1] *= dataset[:, 4]\n","input_t[:, 2] *= dataset[:, 4]\n","answers_t = np.zeros((len(dataset[:, 0]), 3))\n","answers_t[:, 0] = dataset[:, 6] + dataset[:, 7]\n","answers_t[:, 1] = dataset[:, 8]\n","answers_t[:, 2] = dataset[:, 9]"],"metadata":{"id":"JJ0Zucdgpy7y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747924361208,"user_tz":240,"elapsed":8105,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"428fb1b7-30e1-4f14-a786-ab35e6aaefaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(480000, 16)\n"]}]},{"cell_type":"markdown","source":[" Normalize inputs (improve this in future)"],"metadata":{"id":"sl0jgWPh6d59"}},{"cell_type":"code","source":["# normalize inputs\n","input_t2 = input_t.copy()\n","# refractive index and asymmetry parameter linear norm\n","input_t2[:, 0] = input_t[:, 0]/2.5\n","input_t2[:, 3] = input_t[:, 3]\n","# mu_s*t and mu_a*t logrithmic norm\n","input_t2[:, 1] = np.log10(input_t[:, 1]+0.0001)\n","input_t2[:, 1] += 4\n","input_t2[:, 1] /= 8.69154\n","input_t2[:, 2] = np.log10(input_t[:, 2]+0.0001)\n","input_t2[:, 2] += 4\n","input_t2[:, 2] /= 8.69549\n","input_t2[:, 3] = -((-input_t2[:, 3]+1)**(1/3))+1"],"metadata":{"id":"nYkrE4FD6pHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Organize training and test data into 4 sets of tensors by number of layers"],"metadata":{"id":"y2ZxClqTCk4O"}},{"cell_type":"code","source":["input_1l = np.zeros((0, 4))\n","input_2l = np.zeros((0, 4))\n","input_3l = np.zeros((0, 4))\n","input_4l = np.zeros((0, 4))\n","answers_1l = np.zeros((0, 3))\n","answers_2l = np.zeros((0, 3))\n","answers_3l = np.zeros((0, 3))\n","answers_4l = np.zeros((0, 3))\n","\n","for i in range(int(len(input_t2[:, 0])/6)):\n","  index = i*6\n","  # get num layers then organize\n","  if input_t2[index+1, 0] == 0:\n","    input_1l = np.vstack((input_1l, input_t2[index, :]))\n","    answers_1l = np.vstack((answers_1l, answers_t[index, :]))\n","  elif input_t2[index+2, 0] == 0:\n","    input_2l = np.vstack((input_2l, input_t2[index:index+2, :]))\n","    answers_2l = np.vstack((answers_2l, answers_t[index, :]))\n","  elif input_t2[index+3, 0] == 0:\n","    input_3l = np.vstack((input_3l, input_t2[index:index+3, :]))\n","    answers_3l = np.vstack((answers_3l, answers_t[index, :]))\n","  else:\n","    input_4l = np.vstack((input_4l, input_t2[index:index+4, :]))\n","    answers_4l = np.vstack((answers_4l, answers_t[index, :]))\n","# move over to tensor\n","input_1l = torch.from_numpy(input_1l.astype(np.float32))\n","input_2l = torch.from_numpy(input_2l.astype(np.float32))\n","input_3l = torch.from_numpy(input_3l.astype(np.float32))\n","input_4l = torch.from_numpy(input_4l.astype(np.float32))\n","answers_1l = torch.from_numpy(answers_1l.astype(np.float32))\n","answers_2l = torch.from_numpy(answers_2l.astype(np.float32))\n","answers_3l = torch.from_numpy(answers_3l.astype(np.float32))\n","answers_4l = torch.from_numpy(answers_4l.astype(np.float32))\n","# create input tensors (num layers, batch size, num inputs)\n","input_tensor_1l = torch.zeros(1, len(answers_1l[:, 0]), 4)\n","input_tensor_2l = torch.zeros(2, len(answers_2l[:, 0]), 4)\n","input_tensor_3l = torch.zeros(3, len(answers_3l[:, 0]), 4)\n","input_tensor_4l = torch.zeros(4, len(answers_4l[:, 0]), 4)\n","for i in range((len(answers_1l[:, 0]))):\n","  input_tensor_1l[:, i, :] = input_1l[i, :]\n","for i in range((len(answers_2l[:, 0]))):\n","  input_tensor_2l[:, i, :] = input_2l[2*i:(2*i+2), :]\n","for i in range((len(answers_3l[:, 0]))):\n","  input_tensor_3l[:, i, :] = input_3l[3*i:(3*i+3), :]\n","for i in range((len(answers_4l[:, 0]))):\n","  input_tensor_4l[:, i, :] = input_4l[4*i:(4*i+4), :]\n","print(input_tensor_1l.shape)\n","print(input_tensor_2l.shape)\n","print(input_tensor_3l.shape)\n","print(input_tensor_4l.shape)"],"metadata":{"id":"yGOH2EN6Ckhm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747924372188,"user_tz":240,"elapsed":9125,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"db584bce-6f5a-41c1-a400-9b0408a8b8d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 19981, 4])\n","torch.Size([2, 19967, 4])\n","torch.Size([3, 20055, 4])\n","torch.Size([4, 19997, 4])\n"]}]},{"cell_type":"markdown","source":["Also import in new validation datasets for 5 and 6 layers"],"metadata":{"id":"_8IESeKkwpPR"}},{"cell_type":"code","source":["dataset2 = np.loadtxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/D1.2_validation.txt')\n","input_t = np.zeros((len(dataset2[:, 0]), 4))\n","input_t[:, :] = dataset2[:, :4]\n","# dimensionless mu_a*t and mu_s*t\n","input_t[:, 1] *= dataset2[:, 4]\n","input_t[:, 2] *= dataset2[:, 4]\n","answers_t = np.zeros((len(dataset2[:, 0]), 3))\n","answers_t[:, 0] = dataset2[:, 6] + dataset2[:, 7]\n","answers_t[:, 1] = dataset2[:, 8]\n","answers_t[:, 2] = dataset2[:, 9]\n","\n","input_5l = np.zeros((0, 4))\n","input_6l = np.zeros((0, 4))\n","answers_5l = np.zeros((0, 3))\n","answers_6l = np.zeros((0, 3))\n","\n","# normalize inputs\n","input_t2 = input_t.copy()\n","# refractive index and asymmetry parameter linear norm\n","input_t2[:, 0] = input_t[:, 0]/np.max(input_t[:, 0])\n","input_t2[:, 3] = input_t[:, 3]/np.max(input_t[:, 3])\n","# mu_s*t and mu_a*t logrithmic norm\n","input_t2[:, 1] = np.log10(input_t[:, 1]+0.0001)\n","input_t2[:, 1] += 4\n","input_t2[:, 1] /= np.max(input_t2[:, 1])\n","input_t2[:, 2] = np.log10(input_t[:, 2]+0.0001)\n","input_t2[:, 2] += 4\n","input_t2[:, 2] /= np.max(input_t2[:, 2])\n","input_t2[:, 3] = -((-input_t2[:, 3]+1)**(1/3))+1\n","\n","for i in range(int(len(input_t2[:, 0])/6)):\n","  index = i*6\n","  # get num layers then organize\n","  if input_t2[index+5, 0] == 0:\n","    input_5l = np.vstack((input_5l, input_t2[index:index+5, :]))\n","    answers_5l = np.vstack((answers_5l, answers_t[index, :]))\n","  else:\n","    input_6l = np.vstack((input_6l, input_t2[index:index+6, :]))\n","    answers_6l = np.vstack((answers_6l, answers_t[index, :]))\n","# move over to tensor\n","input_5l = torch.from_numpy(input_5l.astype(np.float32))\n","input_6l = torch.from_numpy(input_6l.astype(np.float32))\n","answers_5l = torch.from_numpy(answers_5l.astype(np.float32))\n","answers_6l = torch.from_numpy(answers_6l.astype(np.float32))\n","# create input tensors (num layers, batch size, num inputs)\n","input_tensor_5l = torch.zeros(5, len(answers_5l[:, 0]), 4)\n","input_tensor_6l = torch.zeros(6, len(answers_6l[:, 0]), 4)\n","for i in range((len(answers_5l[:, 0]))):\n","  input_tensor_5l[:, i, :] = input_5l[5*i:(5*i+5), :]\n","for i in range((len(answers_6l[:, 0]))):\n","  input_tensor_6l[:, i, :] = input_6l[6*i:(6*i+6), :]\n","input_tensor_5l_test = input_tensor_5l.narrow(1, 0, 1000)\n","input_tensor_6l_test = input_tensor_6l.narrow(1, 0, 1000)\n","answer_tensor_5l_test = answers_5l.narrow(0, 0, 1000)\n","answer_tensor_6l_test = answers_6l.narrow(0, 0, 1000)"],"metadata":{"id":"oE7TzB0GwsjI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split into train (40,000) and test (4,000) datasets"],"metadata":{"id":"r-vadFudRLLH"}},{"cell_type":"code","source":["# randomize tensors indices\n","np.random.seed(128)\n","\n","input_tensor_1l_rand = torch.zeros_like(input_tensor_1l)\n","input_tensor_2l_rand = torch.zeros_like(input_tensor_2l)\n","input_tensor_3l_rand = torch.zeros_like(input_tensor_3l)\n","input_tensor_4l_rand = torch.zeros_like(input_tensor_4l)\n","answers_1l_rand = torch.zeros_like(answers_1l)\n","answers_2l_rand = torch.zeros_like(answers_2l)\n","answers_3l_rand = torch.zeros_like(answers_3l)\n","answers_4l_rand = torch.zeros_like(answers_4l)\n","\n","rand1 = np.arange(19981)\n","np.random.shuffle(rand1)\n","for i in range(19981):\n","  input_tensor_1l_rand[:, rand1[i], :] = input_tensor_1l[:, i, :]\n","  answers_1l_rand[rand1[i], :] = answers_1l[i, :]\n","rand2 = np.arange(19967)\n","np.random.shuffle(rand2)\n","for i in range(19967):\n","  input_tensor_2l_rand[:, rand2[i], :] = input_tensor_2l[:, i, :]\n","  answers_2l_rand[rand2[i], :] = answers_2l[i, :]\n","rand3 = np.arange(20055)\n","np.random.shuffle(rand3)\n","for i in range(20055):\n","  input_tensor_3l_rand[:, rand3[i], :] = input_tensor_3l[:, i, :]\n","  answers_3l_rand[rand3[i], :] = answers_3l[i, :]\n","rand4 = np.arange(19997)\n","np.random.shuffle(rand4)\n","for i in range(19997):\n","  input_tensor_4l_rand[:, rand4[i], :] = input_tensor_4l[:, i, :]\n","  answers_4l_rand[rand4[i], :] = answers_4l[i, :]\n","\n","# split data to train and test\n","## PREVIOUSLY 60,000 AND 4,500\n","input_tensor_1l_train_temp = input_tensor_1l_rand.narrow(1, 0, 18720)\n","input_tensor_2l_train_temp = input_tensor_2l_rand.narrow(1, 0, 18720)\n","input_tensor_3l_train_temp = input_tensor_3l_rand.narrow(1, 0, 18720)\n","input_tensor_4l_train_temp = input_tensor_4l_rand.narrow(1, 0, 18720)\n","answer_tensor_1l_train = answers_1l_rand.narrow(0, 0, 18720)\n","answer_tensor_2l_train = answers_2l_rand.narrow(0, 0, 18720)\n","answer_tensor_3l_train = answers_3l_rand.narrow(0, 0, 18720)\n","answer_tensor_4l_train = answers_4l_rand.narrow(0, 0, 18720)\n","input_tensor_1l_test_temp = input_tensor_1l_rand.narrow(1, 18720, 1000)\n","input_tensor_2l_test_temp = input_tensor_2l_rand.narrow(1, 18720, 1000)\n","input_tensor_3l_test_temp = input_tensor_3l_rand.narrow(1, 18720, 1000)\n","input_tensor_4l_test_temp = input_tensor_4l_rand.narrow(1, 18720, 1000)\n","answer_tensor_1l_test = answers_1l_rand.narrow(0, 18720, 1000)\n","answer_tensor_2l_test = answers_2l_rand.narrow(0, 18720, 1000)\n","answer_tensor_3l_test = answers_3l_rand.narrow(0, 18720, 1000)\n","answer_tensor_4l_test = answers_4l_rand.narrow(0, 18720, 1000)"],"metadata":{"id":"KgryZEPLRM__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfold layers for all in one input"],"metadata":{"id":"98SyezIGPvLc"}},{"cell_type":"code","source":["# 1 layer\n","input_tensor_1l_train = torch.zeros(18720, 4)\n","input_tensor_1l_test = torch.zeros(1000, 4)\n","for i in range(1):\n","  input_tensor_1l_train[:, i*4:((i+1)*4)] = input_tensor_1l_train_temp[i, :, :]\n","  input_tensor_1l_test[:, i*4:((i+1)*4)] = input_tensor_1l_test_temp[i, :, :]\n","# 2 layers\n","input_tensor_2l_train = torch.zeros(18720, 8)\n","input_tensor_2l_test = torch.zeros(1000, 8)\n","for i in range(2):\n","  input_tensor_2l_train[:, i*4:((i+1)*4)] = input_tensor_2l_train_temp[i, :, :]\n","  input_tensor_2l_test[:, i*4:((i+1)*4)] = input_tensor_2l_test_temp[i, :, :]\n","# 3 layers\n","input_tensor_3l_train = torch.zeros(18720, 12)\n","input_tensor_3l_test = torch.zeros(1000, 12)\n","for i in range(3):\n","  input_tensor_3l_train[:, i*4:((i+1)*4)] = input_tensor_3l_train_temp[i, :, :]\n","  input_tensor_3l_test[:, i*4:((i+1)*4)] = input_tensor_3l_test_temp[i, :, :]\n","# 4 layers\n","input_tensor_4l_train = torch.zeros(18720, 16)\n","input_tensor_4l_test = torch.zeros(1000, 16)\n","for i in range(4):\n","  input_tensor_4l_train[:, i*4:((i+1)*4)] = input_tensor_4l_train_temp[i, :, :]\n","  input_tensor_4l_test[:, i*4:((i+1)*4)] = input_tensor_4l_test_temp[i, :, :]"],"metadata":{"id":"mGmIUk06Pzn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_layer, hidden_layer2, output_size):\n","    super(RNN, self).__init__()\n","    self.m2o = nn.Linear(hidden_layer2, output_size)\n","    self.i2l1 = nn.Linear(input_size, hidden_layer)\n","    self.l1l2 = nn.Linear(hidden_layer, hidden_layer)\n","    self.l2l3 = nn.Linear(hidden_layer, hidden_layer2)\n","    self.activ = nn.ReLU()\n","\n","  def forward(self, input_tensor):\n","    mid1 = self.i2l1(input_tensor)\n","    mid1 = self.activ(mid1)\n","    mid2 = self.l1l2(mid1)\n","    mid2 = self.activ(mid2)\n","    mid3 = self.l2l3(mid2)\n","    mid3 = self.activ(mid3)\n","    output = self.m2o(mid3)\n","    return output\n","\n"],"metadata":{"id":"0Ua1Z4sfDmCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(rnn, input_tensor1, answer_tensor1, criterion, optimizer, batch_size, device):\n","  rnn.train()\n","  input_tensor1 = input_tensor1.to(device)\n","  answer_tensor1 = answer_tensor1.to(device)\n","\n","  optimizer.zero_grad()\n","  # feedforward\n","  output1 = rnn.forward(input_tensor1)\n","\n","  loss = criterion(output1, answer_tensor1)\n","  loss.backward()\n","  optimizer.step()\n","  return output1, loss.item()"],"metadata":{"id":"d3M3ZI_yNNXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(rnn, input_tensor, answer_tensor, criterion, optimizer, batch_size, device):\n","  rnn.eval()\n","  input_tensor = input_tensor.to(device)\n","  answer_tensor = answer_tensor.to(device)\n","  # get initial hidden state\n","  output1 = rnn.forward(input_tensor)\n","\n","  loss = criterion(output1, answer_tensor)\n","  return output1, loss.item()"],"metadata":{"id":"-fx6qOdUQFNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run to train RNN"],"metadata":{"id":"KOgFx8WtFmRW"}},{"cell_type":"code","source":["np.random.seed(128)\n","torch.manual_seed(128)\n","input_dim = 4\n","# hyperparameters to tune\n","hidden_layer = 1024\n","hidden_layer2 = 1024\n","\n","# output is R, A, T\n","output_size = 3\n","batch_size = 585\n","batch_size_test = 1240\n","num_batches = int(18720/batch_size)\n","epochs = 1000\n","\n","rnn = RNN(input_dim, hidden_layer, hidden_layer2, output_size)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Training on: \" + str(device))\n","rnn = rnn.to(device)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n","current_loss_train = 0\n","current_loss_test = 0\n","loss_plot_test = np.zeros(epochs)\n","loss_plot_train = np.zeros(epochs*num_batches)\n","x_axis = np.linspace(0, epochs, num=epochs*num_batches)\n","\n","\n","for epoch in range(epochs):\n","  for batch in range(num_batches):\n","    # one batch from each layer\n","    output, loss = train(rnn, input_tensor_1l_train[batch_size*batch:(batch_size*(batch+1)), :], answer_tensor_1l_train[batch_size*batch:(batch_size*(batch+1)), :], criterion, optimizer, batch_size, device)\n","    current_loss_train += loss\n","    loss_plot_train[batch + epoch*num_batches] = current_loss_train\n","    current_loss_train = 0\n","  # validation\n","  output, loss = test(rnn, input_tensor_1l_test, answer_tensor_1l_test, criterion, optimizer, batch_size_test, device)\n","  current_loss_test += loss\n","\n","  print(epoch, \"/\", epochs, \"Val loss:\", current_loss_test)\n","  loss_plot_test[epoch] = current_loss_test\n","  current_loss_test = 0\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zu2kPmrjIcnJ","executionInfo":{"status":"ok","timestamp":1747924484340,"user_tz":240,"elapsed":97554,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"0097ffb3-ab8b-48a1-9129-0cb1ca4e0825"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on: cuda\n","0 / 1000 Val loss: 0.030489347875118256\n","1 / 1000 Val loss: 0.014242489822208881\n","2 / 1000 Val loss: 0.00857547577470541\n","3 / 1000 Val loss: 0.005897488910704851\n","4 / 1000 Val loss: 0.003872124245390296\n","5 / 1000 Val loss: 0.0023509846068918705\n","6 / 1000 Val loss: 0.001411503297276795\n","7 / 1000 Val loss: 0.0009289028821513057\n","8 / 1000 Val loss: 0.0006488139042630792\n","9 / 1000 Val loss: 0.0004769889055751264\n","10 / 1000 Val loss: 0.0003677746281027794\n","11 / 1000 Val loss: 0.000299190724035725\n","12 / 1000 Val loss: 0.00025236920919269323\n","13 / 1000 Val loss: 0.00021730863954871893\n","14 / 1000 Val loss: 0.0001888750120997429\n","15 / 1000 Val loss: 0.00016495358431711793\n","16 / 1000 Val loss: 0.00014481935068033636\n","17 / 1000 Val loss: 0.00012914740364067256\n","18 / 1000 Val loss: 0.0001182790074381046\n","19 / 1000 Val loss: 0.00011055367212975398\n","20 / 1000 Val loss: 0.00010263443982694298\n","21 / 1000 Val loss: 9.39573219511658e-05\n","22 / 1000 Val loss: 8.637701830593869e-05\n","23 / 1000 Val loss: 8.060028631007299e-05\n","24 / 1000 Val loss: 7.582479884149507e-05\n","25 / 1000 Val loss: 7.113550236681476e-05\n","26 / 1000 Val loss: 6.726452556904405e-05\n","27 / 1000 Val loss: 6.442599988076836e-05\n","28 / 1000 Val loss: 6.218282214831561e-05\n","29 / 1000 Val loss: 5.9561600210145116e-05\n","30 / 1000 Val loss: 5.5601060012122616e-05\n","31 / 1000 Val loss: 5.1369526772759855e-05\n","32 / 1000 Val loss: 4.810167956748046e-05\n","33 / 1000 Val loss: 4.6119534090394154e-05\n","34 / 1000 Val loss: 4.4637254177359864e-05\n","35 / 1000 Val loss: 4.3474166886880994e-05\n","36 / 1000 Val loss: 4.295277540222742e-05\n","37 / 1000 Val loss: 4.2483916331548244e-05\n","38 / 1000 Val loss: 4.122146128793247e-05\n","39 / 1000 Val loss: 3.870068758260459e-05\n","40 / 1000 Val loss: 3.560974801075645e-05\n","41 / 1000 Val loss: 3.28932874253951e-05\n","42 / 1000 Val loss: 3.1247811421053484e-05\n","43 / 1000 Val loss: 3.0035456802579574e-05\n","44 / 1000 Val loss: 2.9001719667576253e-05\n","45 / 1000 Val loss: 2.7929499992751516e-05\n","46 / 1000 Val loss: 2.7095105906482786e-05\n","47 / 1000 Val loss: 2.6951387553708628e-05\n","48 / 1000 Val loss: 2.68165076704463e-05\n","49 / 1000 Val loss: 2.5583487513358705e-05\n","50 / 1000 Val loss: 2.3891430828371085e-05\n","51 / 1000 Val loss: 2.278626743645873e-05\n","52 / 1000 Val loss: 2.208169098594226e-05\n","53 / 1000 Val loss: 2.1307185306795873e-05\n","54 / 1000 Val loss: 2.066197521344293e-05\n","55 / 1000 Val loss: 2.072807001241017e-05\n","56 / 1000 Val loss: 2.134288297384046e-05\n","57 / 1000 Val loss: 2.11642800422851e-05\n","58 / 1000 Val loss: 2.0748913811985403e-05\n","59 / 1000 Val loss: 2.1806510630995035e-05\n","60 / 1000 Val loss: 2.298503022757359e-05\n","61 / 1000 Val loss: 2.451945147186052e-05\n","62 / 1000 Val loss: 2.6185722163063474e-05\n","63 / 1000 Val loss: 2.6290093956049532e-05\n","64 / 1000 Val loss: 2.5949591872631572e-05\n","65 / 1000 Val loss: 2.543338996474631e-05\n","66 / 1000 Val loss: 2.5344315872644074e-05\n","67 / 1000 Val loss: 2.563184898463078e-05\n","68 / 1000 Val loss: 2.5761399228940718e-05\n","69 / 1000 Val loss: 2.583631248853635e-05\n","70 / 1000 Val loss: 2.6080462703248486e-05\n","71 / 1000 Val loss: 2.7190753826289438e-05\n","72 / 1000 Val loss: 2.814355138980318e-05\n","73 / 1000 Val loss: 2.812579805322457e-05\n","74 / 1000 Val loss: 2.728908475546632e-05\n","75 / 1000 Val loss: 2.538828266551718e-05\n","76 / 1000 Val loss: 2.4292628950206563e-05\n","77 / 1000 Val loss: 2.3481208700104617e-05\n","78 / 1000 Val loss: 2.290972406626679e-05\n","79 / 1000 Val loss: 2.177958049287554e-05\n","80 / 1000 Val loss: 2.072373899864033e-05\n","81 / 1000 Val loss: 1.8874770830734633e-05\n","82 / 1000 Val loss: 1.728055576677434e-05\n","83 / 1000 Val loss: 1.499908739788225e-05\n","84 / 1000 Val loss: 1.3237538951216266e-05\n","85 / 1000 Val loss: 1.1321628335281275e-05\n","86 / 1000 Val loss: 1.0147829925699625e-05\n","87 / 1000 Val loss: 9.740132554725278e-06\n","88 / 1000 Val loss: 9.577369382895995e-06\n","89 / 1000 Val loss: 9.58160399022745e-06\n","90 / 1000 Val loss: 1.0353532161389012e-05\n","91 / 1000 Val loss: 2.491378108970821e-05\n","92 / 1000 Val loss: 1.3805519301968161e-05\n","93 / 1000 Val loss: 1.6275738744297996e-05\n","94 / 1000 Val loss: 9.087544640351553e-06\n","95 / 1000 Val loss: 9.901665180223063e-06\n","96 / 1000 Val loss: 9.442429472983349e-06\n","97 / 1000 Val loss: 1.0145071428269148e-05\n","98 / 1000 Val loss: 9.816319106903393e-06\n","99 / 1000 Val loss: 1.52834127220558e-05\n","100 / 1000 Val loss: 9.059410331246909e-06\n","101 / 1000 Val loss: 9.897321433527395e-06\n","102 / 1000 Val loss: 1.173321834357921e-05\n","103 / 1000 Val loss: 2.2306696337182075e-05\n","104 / 1000 Val loss: 1.126633014791878e-05\n","105 / 1000 Val loss: 2.3628463168279268e-05\n","106 / 1000 Val loss: 2.084531843138393e-05\n","107 / 1000 Val loss: 9.215623322234023e-06\n","108 / 1000 Val loss: 2.6230973162455484e-05\n","109 / 1000 Val loss: 1.1284463653282728e-05\n","110 / 1000 Val loss: 1.0546483281359542e-05\n","111 / 1000 Val loss: 1.2404540029820055e-05\n","112 / 1000 Val loss: 1.0770281733130105e-05\n","113 / 1000 Val loss: 8.66343907546252e-06\n","114 / 1000 Val loss: 1.4772255781281274e-05\n","115 / 1000 Val loss: 1.4067782103666104e-05\n","116 / 1000 Val loss: 1.3745779142482206e-05\n","117 / 1000 Val loss: 1.5602032362949103e-05\n","118 / 1000 Val loss: 1.4520664990413934e-05\n","119 / 1000 Val loss: 7.959542926982976e-06\n","120 / 1000 Val loss: 7.96260701463325e-06\n","121 / 1000 Val loss: 8.921886546886526e-06\n","122 / 1000 Val loss: 9.922259778250009e-06\n","123 / 1000 Val loss: 9.305919775215443e-06\n","124 / 1000 Val loss: 9.869305358733982e-06\n","125 / 1000 Val loss: 1.2153283023508266e-05\n","126 / 1000 Val loss: 1.0839478818525095e-05\n","127 / 1000 Val loss: 1.1882914805028122e-05\n","128 / 1000 Val loss: 1.219732530444162e-05\n","129 / 1000 Val loss: 2.175882036681287e-05\n","130 / 1000 Val loss: 6.644464065175271e-06\n","131 / 1000 Val loss: 7.536494649684755e-06\n","132 / 1000 Val loss: 1.0131721865036525e-05\n","133 / 1000 Val loss: 1.0924308298854157e-05\n","134 / 1000 Val loss: 1.1619179531408008e-05\n","135 / 1000 Val loss: 1.1965295925620012e-05\n","136 / 1000 Val loss: 1.2756967407767661e-05\n","137 / 1000 Val loss: 1.4012861356604844e-05\n","138 / 1000 Val loss: 1.0885864867304917e-05\n","139 / 1000 Val loss: 1.237346077687107e-05\n","140 / 1000 Val loss: 1.5514200640609488e-05\n","141 / 1000 Val loss: 1.1888687367900275e-05\n","142 / 1000 Val loss: 1.0427073902974371e-05\n","143 / 1000 Val loss: 7.7025979408063e-06\n","144 / 1000 Val loss: 7.881052624725271e-06\n","145 / 1000 Val loss: 8.519915354554541e-06\n","146 / 1000 Val loss: 2.0945060896337964e-05\n","147 / 1000 Val loss: 9.242186933988705e-06\n","148 / 1000 Val loss: 4.696054747910239e-05\n","149 / 1000 Val loss: 2.562685767770745e-05\n","150 / 1000 Val loss: 1.037644688040018e-05\n","151 / 1000 Val loss: 9.634275556891225e-06\n","152 / 1000 Val loss: 6.781990123272408e-06\n","153 / 1000 Val loss: 7.886588718974963e-06\n","154 / 1000 Val loss: 7.530955826950958e-06\n","155 / 1000 Val loss: 6.48895093036117e-06\n","156 / 1000 Val loss: 5.810700258734869e-06\n","157 / 1000 Val loss: 6.757379651389783e-06\n","158 / 1000 Val loss: 5.647916623274796e-06\n","159 / 1000 Val loss: 6.193994067871245e-06\n","160 / 1000 Val loss: 0.0001503048260929063\n","161 / 1000 Val loss: 4.080044891452417e-05\n","162 / 1000 Val loss: 2.9835857276339084e-05\n","163 / 1000 Val loss: 1.7662654499872588e-05\n","164 / 1000 Val loss: 1.2735514246742241e-05\n","165 / 1000 Val loss: 1.2590662663569674e-05\n","166 / 1000 Val loss: 1.3173580555303488e-05\n","167 / 1000 Val loss: 1.357278506475268e-05\n","168 / 1000 Val loss: 1.4270594874687959e-05\n","169 / 1000 Val loss: 1.4026952158019412e-05\n","170 / 1000 Val loss: 1.3898656106903218e-05\n","171 / 1000 Val loss: 1.301439078815747e-05\n","172 / 1000 Val loss: 1.0259881491947453e-05\n","173 / 1000 Val loss: 7.982450370036531e-06\n","174 / 1000 Val loss: 7.573736638732953e-06\n","175 / 1000 Val loss: 8.817495654511731e-06\n","176 / 1000 Val loss: 8.117100151139311e-06\n","177 / 1000 Val loss: 7.036456736386754e-06\n","178 / 1000 Val loss: 6.4488208408874925e-06\n","179 / 1000 Val loss: 7.806162102497183e-06\n","180 / 1000 Val loss: 8.319455446326174e-06\n","181 / 1000 Val loss: 1.3112208762322553e-05\n","182 / 1000 Val loss: 6.5550420913496055e-06\n","183 / 1000 Val loss: 7.308721251320094e-06\n","184 / 1000 Val loss: 1.2165943189756945e-05\n","185 / 1000 Val loss: 9.155764928436838e-06\n","186 / 1000 Val loss: 1.1427811841713265e-05\n","187 / 1000 Val loss: 1.2744108062179293e-05\n","188 / 1000 Val loss: 1.3802191460854374e-05\n","189 / 1000 Val loss: 1.7687611034489237e-05\n","190 / 1000 Val loss: 1.3552117707149591e-05\n","191 / 1000 Val loss: 1.0904335795203224e-05\n","192 / 1000 Val loss: 7.3931746555899736e-06\n","193 / 1000 Val loss: 1.086739302991191e-05\n","194 / 1000 Val loss: 8.364655514014885e-06\n","195 / 1000 Val loss: 6.337322702165693e-06\n","196 / 1000 Val loss: 4.976792752131587e-06\n","197 / 1000 Val loss: 5.348791091819294e-06\n","198 / 1000 Val loss: 7.174071015469963e-06\n","199 / 1000 Val loss: 6.554567789862631e-06\n","200 / 1000 Val loss: 4.226349119562656e-05\n","201 / 1000 Val loss: 2.7277872504782863e-05\n","202 / 1000 Val loss: 1.0487819054105785e-05\n","203 / 1000 Val loss: 1.0639960237313062e-05\n","204 / 1000 Val loss: 1.1753770195355173e-05\n","205 / 1000 Val loss: 1.1855271623062436e-05\n","206 / 1000 Val loss: 1.1332461326674093e-05\n","207 / 1000 Val loss: 9.107223377213813e-06\n","208 / 1000 Val loss: 7.448747510352405e-06\n","209 / 1000 Val loss: 7.3867340688593686e-06\n","210 / 1000 Val loss: 1.300251005886821e-05\n","211 / 1000 Val loss: 1.500771668361267e-05\n","212 / 1000 Val loss: 1.531693123979494e-05\n","213 / 1000 Val loss: 6.7728874455497134e-06\n","214 / 1000 Val loss: 1.148533101513749e-05\n","215 / 1000 Val loss: 1.3504764865501784e-05\n","216 / 1000 Val loss: 3.50652371707838e-05\n","217 / 1000 Val loss: 2.3277059881365858e-05\n","218 / 1000 Val loss: 6.7739529185928404e-06\n","219 / 1000 Val loss: 1.0380749699834269e-05\n","220 / 1000 Val loss: 9.537029654893558e-06\n","221 / 1000 Val loss: 7.885558261477854e-06\n","222 / 1000 Val loss: 7.074735094647622e-06\n","223 / 1000 Val loss: 6.384033440554049e-06\n","224 / 1000 Val loss: 5.154763584869215e-06\n","225 / 1000 Val loss: 4.766523488797247e-06\n","226 / 1000 Val loss: 6.076979843783192e-06\n","227 / 1000 Val loss: 1.4574006854672916e-05\n","228 / 1000 Val loss: 1.4864473087072838e-05\n","229 / 1000 Val loss: 9.265895641874522e-06\n","230 / 1000 Val loss: 5.827156655868748e-06\n","231 / 1000 Val loss: 2.008024057431612e-05\n","232 / 1000 Val loss: 8.097161298792344e-06\n","233 / 1000 Val loss: 5.320965101418551e-06\n","234 / 1000 Val loss: 7.5617185757437255e-06\n","235 / 1000 Val loss: 1.706112379906699e-05\n","236 / 1000 Val loss: 1.2629388038476463e-05\n","237 / 1000 Val loss: 8.946863090386614e-06\n","238 / 1000 Val loss: 1.5589648683089763e-05\n","239 / 1000 Val loss: 6.757141363777919e-06\n","240 / 1000 Val loss: 6.978092642384581e-06\n","241 / 1000 Val loss: 6.0169836615386885e-06\n","242 / 1000 Val loss: 6.184267022035783e-06\n","243 / 1000 Val loss: 5.576670446316712e-06\n","244 / 1000 Val loss: 4.748617811856093e-06\n","245 / 1000 Val loss: 4.2149667933699675e-06\n","246 / 1000 Val loss: 4.143034675507806e-05\n","247 / 1000 Val loss: 6.881643002998317e-06\n","248 / 1000 Val loss: 9.943502845999319e-06\n","249 / 1000 Val loss: 5.29485851075151e-06\n","250 / 1000 Val loss: 6.498372840724187e-06\n","251 / 1000 Val loss: 1.33749354063184e-05\n","252 / 1000 Val loss: 3.626808029366657e-05\n","253 / 1000 Val loss: 3.249155997764319e-05\n","254 / 1000 Val loss: 1.526439518784173e-05\n","255 / 1000 Val loss: 1.4641273082816042e-05\n","256 / 1000 Val loss: 1.1363200428604614e-05\n","257 / 1000 Val loss: 1.1118970178358722e-05\n","258 / 1000 Val loss: 1.0564025615167338e-05\n","259 / 1000 Val loss: 1.1155263564432971e-05\n","260 / 1000 Val loss: 1.107632397179259e-05\n","261 / 1000 Val loss: 1.0841824405360967e-05\n","262 / 1000 Val loss: 1.1388090570108034e-05\n","263 / 1000 Val loss: 1.4468138942902442e-05\n","264 / 1000 Val loss: 1.1462834663689137e-05\n","265 / 1000 Val loss: 5.236134529695846e-05\n","266 / 1000 Val loss: 1.4703172382724006e-05\n","267 / 1000 Val loss: 2.603077700769063e-05\n","268 / 1000 Val loss: 4.011265991721302e-05\n","269 / 1000 Val loss: 9.930477062880527e-06\n","270 / 1000 Val loss: 6.64261688143597e-06\n","271 / 1000 Val loss: 7.811349860276096e-06\n","272 / 1000 Val loss: 1.088525550585473e-05\n","273 / 1000 Val loss: 1.2558449270727579e-05\n","274 / 1000 Val loss: 1.4193805327522568e-05\n","275 / 1000 Val loss: 1.3498978660209104e-05\n","276 / 1000 Val loss: 1.1586283108044881e-05\n","277 / 1000 Val loss: 1.089033958123764e-05\n","278 / 1000 Val loss: 1.207627519761445e-05\n","279 / 1000 Val loss: 6.3741535996086895e-06\n","280 / 1000 Val loss: 4.58762997368467e-06\n","281 / 1000 Val loss: 3.954225121560739e-06\n","282 / 1000 Val loss: 3.971228579757735e-06\n","283 / 1000 Val loss: 4.582091150950873e-06\n","284 / 1000 Val loss: 4.800367150892271e-06\n","285 / 1000 Val loss: 4.554581209958997e-06\n","286 / 1000 Val loss: 7.795628334861249e-06\n","287 / 1000 Val loss: 1.6251369743258692e-05\n","288 / 1000 Val loss: 1.2655270438699517e-05\n","289 / 1000 Val loss: 7.863082828407642e-06\n","290 / 1000 Val loss: 8.856854947225656e-06\n","291 / 1000 Val loss: 6.985276286286535e-06\n","292 / 1000 Val loss: 5.068457539891824e-06\n","293 / 1000 Val loss: 6.304788712441223e-06\n","294 / 1000 Val loss: 9.011008842207957e-06\n","295 / 1000 Val loss: 1.4741995073563885e-05\n","296 / 1000 Val loss: 1.9836401406791992e-05\n","297 / 1000 Val loss: 1.0061994544230402e-05\n","298 / 1000 Val loss: 7.5610778367263265e-06\n","299 / 1000 Val loss: 8.043075467867311e-06\n","300 / 1000 Val loss: 7.847909728297964e-06\n","301 / 1000 Val loss: 9.645575119066052e-06\n","302 / 1000 Val loss: 1.415601946064271e-05\n","303 / 1000 Val loss: 1.1704031749104615e-05\n","304 / 1000 Val loss: 1.5641904610674828e-05\n","305 / 1000 Val loss: 9.495644917478785e-06\n","306 / 1000 Val loss: 1.2511507520684972e-05\n","307 / 1000 Val loss: 7.923556950117927e-06\n","308 / 1000 Val loss: 6.645406301686307e-06\n","309 / 1000 Val loss: 5.44112754141679e-06\n","310 / 1000 Val loss: 1.0789240150188562e-05\n","311 / 1000 Val loss: 1.8861694115912542e-05\n","312 / 1000 Val loss: 2.6434670871822163e-05\n","313 / 1000 Val loss: 1.794043419067748e-05\n","314 / 1000 Val loss: 1.7030653907568194e-05\n","315 / 1000 Val loss: 1.3024233339820057e-05\n","316 / 1000 Val loss: 1.2895052350359038e-05\n","317 / 1000 Val loss: 7.887897481850814e-06\n","318 / 1000 Val loss: 7.142855338315712e-06\n","319 / 1000 Val loss: 1.0225071491731796e-05\n","320 / 1000 Val loss: 2.5094514057855122e-05\n","321 / 1000 Val loss: 9.98948235064745e-06\n","322 / 1000 Val loss: 2.426189894322306e-05\n","323 / 1000 Val loss: 1.6109730495372787e-05\n","324 / 1000 Val loss: 1.0297790140612051e-05\n","325 / 1000 Val loss: 1.015449652186362e-05\n","326 / 1000 Val loss: 1.234930368809728e-05\n","327 / 1000 Val loss: 1.391752994095441e-05\n","328 / 1000 Val loss: 1.543719736218918e-05\n","329 / 1000 Val loss: 1.7624626707402058e-05\n","330 / 1000 Val loss: 2.010891148529481e-05\n","331 / 1000 Val loss: 3.369996920810081e-05\n","332 / 1000 Val loss: 2.187166319345124e-05\n","333 / 1000 Val loss: 1.4874145563226193e-05\n","334 / 1000 Val loss: 1.6836522263474762e-05\n","335 / 1000 Val loss: 3.59250589099247e-05\n","336 / 1000 Val loss: 4.114053808734752e-05\n","337 / 1000 Val loss: 6.764693807781441e-06\n","338 / 1000 Val loss: 8.468822670693044e-06\n","339 / 1000 Val loss: 6.7602345552586485e-06\n","340 / 1000 Val loss: 6.09673907092656e-06\n","341 / 1000 Val loss: 4.597296538122464e-06\n","342 / 1000 Val loss: 4.0617910599394236e-06\n","343 / 1000 Val loss: 3.881666998495348e-06\n","344 / 1000 Val loss: 4.242003342369571e-06\n","345 / 1000 Val loss: 4.825603809877066e-06\n","346 / 1000 Val loss: 5.413379312813049e-06\n","347 / 1000 Val loss: 6.0477605074993335e-06\n","348 / 1000 Val loss: 6.623117769777309e-06\n","349 / 1000 Val loss: 7.36355787012144e-06\n","350 / 1000 Val loss: 7.88474062574096e-06\n","351 / 1000 Val loss: 8.887444892025087e-06\n","352 / 1000 Val loss: 1.0587892575131264e-05\n","353 / 1000 Val loss: 9.546763976686634e-06\n","354 / 1000 Val loss: 1.1897255717485677e-05\n","355 / 1000 Val loss: 5.638047696265858e-06\n","356 / 1000 Val loss: 1.0696462595660705e-05\n","357 / 1000 Val loss: 5.2462282837950625e-06\n","358 / 1000 Val loss: 7.191871191025712e-06\n","359 / 1000 Val loss: 1.4633497812610585e-05\n","360 / 1000 Val loss: 1.4370612007041927e-05\n","361 / 1000 Val loss: 1.8882847143686377e-05\n","362 / 1000 Val loss: 1.259033251699293e-05\n","363 / 1000 Val loss: 1.2642569345189258e-05\n","364 / 1000 Val loss: 1.3144225704309065e-05\n","365 / 1000 Val loss: 1.2218772099004127e-05\n","366 / 1000 Val loss: 1.102114129025722e-05\n","367 / 1000 Val loss: 1.0448898137838114e-05\n","368 / 1000 Val loss: 1.0599446795822587e-05\n","369 / 1000 Val loss: 1.0295094398315996e-05\n","370 / 1000 Val loss: 1.4515178008878138e-05\n","371 / 1000 Val loss: 1.3466675227391534e-05\n","372 / 1000 Val loss: 1.678626358625479e-05\n","373 / 1000 Val loss: 1.045645785779925e-05\n","374 / 1000 Val loss: 2.159361793019343e-05\n","375 / 1000 Val loss: 1.9004224668606184e-05\n","376 / 1000 Val loss: 2.2727706891600974e-05\n","377 / 1000 Val loss: 2.533598853915464e-05\n","378 / 1000 Val loss: 3.268820364610292e-05\n","379 / 1000 Val loss: 7.1642152761342e-06\n","380 / 1000 Val loss: 1.4829964129603468e-05\n","381 / 1000 Val loss: 3.6574906516761985e-06\n","382 / 1000 Val loss: 4.123807684663916e-06\n","383 / 1000 Val loss: 4.049142262374517e-06\n","384 / 1000 Val loss: 4.377719960757531e-06\n","385 / 1000 Val loss: 4.083778094354784e-06\n","386 / 1000 Val loss: 4.7264620661735535e-06\n","387 / 1000 Val loss: 1.077130036719609e-05\n","388 / 1000 Val loss: 9.84967209660681e-06\n","389 / 1000 Val loss: 7.033318524918286e-06\n","390 / 1000 Val loss: 3.974407718487782e-06\n","391 / 1000 Val loss: 3.5250395740149543e-06\n","392 / 1000 Val loss: 3.678941084217513e-06\n","393 / 1000 Val loss: 6.7114779085386544e-06\n","394 / 1000 Val loss: 1.542246354802046e-05\n","395 / 1000 Val loss: 5.4577567425440066e-06\n","396 / 1000 Val loss: 5.3732846936327405e-06\n","397 / 1000 Val loss: 4.131150944886031e-06\n","398 / 1000 Val loss: 4.054288638144499e-06\n","399 / 1000 Val loss: 4.114236162422458e-06\n","400 / 1000 Val loss: 4.31773560194415e-06\n","401 / 1000 Val loss: 4.765489393321332e-06\n","402 / 1000 Val loss: 5.975665317237144e-06\n","403 / 1000 Val loss: 1.3194417988415807e-05\n","404 / 1000 Val loss: 1.1135197382827755e-05\n","405 / 1000 Val loss: 8.963767868408468e-06\n","406 / 1000 Val loss: 1.4072931662667543e-05\n","407 / 1000 Val loss: 1.3514187230612151e-05\n","408 / 1000 Val loss: 1.3541216503654141e-05\n","409 / 1000 Val loss: 5.769340987171745e-06\n","410 / 1000 Val loss: 4.343904947745614e-06\n","411 / 1000 Val loss: 5.689149929821724e-06\n","412 / 1000 Val loss: 7.607554834976327e-06\n","413 / 1000 Val loss: 1.1265390639891848e-05\n","414 / 1000 Val loss: 1.4528814972436521e-05\n","415 / 1000 Val loss: 1.722718843666371e-05\n","416 / 1000 Val loss: 1.925428477989044e-05\n","417 / 1000 Val loss: 2.785559991025366e-05\n","418 / 1000 Val loss: 5.369362042983994e-06\n","419 / 1000 Val loss: 5.304442765918793e-06\n","420 / 1000 Val loss: 8.308454198413529e-06\n","421 / 1000 Val loss: 2.3061458705342375e-05\n","422 / 1000 Val loss: 2.1538728105952032e-05\n","423 / 1000 Val loss: 4.803888259630185e-06\n","424 / 1000 Val loss: 6.117295470176032e-06\n","425 / 1000 Val loss: 4.755129793920787e-06\n","426 / 1000 Val loss: 5.4790370995760895e-06\n","427 / 1000 Val loss: 9.013162525661755e-06\n","428 / 1000 Val loss: 2.634190059325192e-05\n","429 / 1000 Val loss: 1.984924529097043e-05\n","430 / 1000 Val loss: 5.12483302372857e-06\n","431 / 1000 Val loss: 7.210205239971401e-06\n","432 / 1000 Val loss: 2.6449833967490122e-05\n","433 / 1000 Val loss: 1.6397187209804542e-05\n","434 / 1000 Val loss: 8.117644938465673e-06\n","435 / 1000 Val loss: 1.1246756002947222e-05\n","436 / 1000 Val loss: 1.052030984283192e-05\n","437 / 1000 Val loss: 9.48002707445994e-06\n","438 / 1000 Val loss: 9.351845619676169e-06\n","439 / 1000 Val loss: 9.238448001269717e-06\n","440 / 1000 Val loss: 8.928445822675712e-06\n","441 / 1000 Val loss: 8.418847755820025e-06\n","442 / 1000 Val loss: 7.423533133987803e-06\n","443 / 1000 Val loss: 6.5577378336456604e-06\n","444 / 1000 Val loss: 5.757405688200379e-06\n","445 / 1000 Val loss: 5.5585146583325695e-06\n","446 / 1000 Val loss: 5.010464519727975e-06\n","447 / 1000 Val loss: 4.376719971332932e-06\n","448 / 1000 Val loss: 3.7289235024218215e-06\n","449 / 1000 Val loss: 3.3982807963184314e-06\n","450 / 1000 Val loss: 3.108235887339106e-06\n","451 / 1000 Val loss: 3.1339636734628584e-06\n","452 / 1000 Val loss: 2.982067144330358e-06\n","453 / 1000 Val loss: 3.070646698688506e-06\n","454 / 1000 Val loss: 5.067903657618444e-06\n","455 / 1000 Val loss: 1.5401581549667753e-05\n","456 / 1000 Val loss: 8.557680303056259e-06\n","457 / 1000 Val loss: 2.7194455469725654e-05\n","458 / 1000 Val loss: 7.2178227128461e-06\n","459 / 1000 Val loss: 1.296243499382399e-05\n","460 / 1000 Val loss: 4.507498033490265e-06\n","461 / 1000 Val loss: 3.7419940781546757e-06\n","462 / 1000 Val loss: 3.1904721708997386e-06\n","463 / 1000 Val loss: 2.7323287667968543e-06\n","464 / 1000 Val loss: 4.3072450353065506e-06\n","465 / 1000 Val loss: 1.469731614633929e-05\n","466 / 1000 Val loss: 1.261002580577042e-05\n","467 / 1000 Val loss: 4.096061275049578e-06\n","468 / 1000 Val loss: 4.919171715300763e-06\n","469 / 1000 Val loss: 4.0873769648897e-06\n","470 / 1000 Val loss: 4.04630191042088e-06\n","471 / 1000 Val loss: 8.35515766084427e-06\n","472 / 1000 Val loss: 8.42363624542486e-06\n","473 / 1000 Val loss: 7.937113878142554e-06\n","474 / 1000 Val loss: 6.31831471764599e-06\n","475 / 1000 Val loss: 6.4453683989995625e-06\n","476 / 1000 Val loss: 2.2472160708275624e-05\n","477 / 1000 Val loss: 1.692661317065358e-05\n","478 / 1000 Val loss: 2.4384833523072302e-05\n","479 / 1000 Val loss: 1.4695391655550338e-05\n","480 / 1000 Val loss: 3.1726256111141993e-06\n","481 / 1000 Val loss: 4.337833161116578e-06\n","482 / 1000 Val loss: 6.042384029569803e-06\n","483 / 1000 Val loss: 6.8113981797068845e-06\n","484 / 1000 Val loss: 7.193164037744282e-06\n","485 / 1000 Val loss: 7.663014002901036e-06\n","486 / 1000 Val loss: 6.963872237975011e-06\n","487 / 1000 Val loss: 7.311873559956439e-06\n","488 / 1000 Val loss: 1.2758384400513023e-05\n","489 / 1000 Val loss: 3.073018524446525e-05\n","490 / 1000 Val loss: 5.2153580327285454e-05\n","491 / 1000 Val loss: 6.6944389800482895e-06\n","492 / 1000 Val loss: 3.2940372420853237e-06\n","493 / 1000 Val loss: 3.20438766721054e-06\n","494 / 1000 Val loss: 3.0948583571444033e-06\n","495 / 1000 Val loss: 3.3100095606641844e-06\n","496 / 1000 Val loss: 3.941656814276939e-06\n","497 / 1000 Val loss: 4.4666812755167484e-06\n","498 / 1000 Val loss: 4.309874384489376e-06\n","499 / 1000 Val loss: 4.211557097733021e-06\n","500 / 1000 Val loss: 6.385479082382517e-06\n","501 / 1000 Val loss: 8.75721343618352e-06\n","502 / 1000 Val loss: 4.863835329160793e-06\n","503 / 1000 Val loss: 7.6136475399835035e-06\n","504 / 1000 Val loss: 5.442846486403141e-06\n","505 / 1000 Val loss: 1.4088922398514114e-05\n","506 / 1000 Val loss: 1.381781476084143e-05\n","507 / 1000 Val loss: 1.482672814745456e-05\n","508 / 1000 Val loss: 6.595120794372633e-06\n","509 / 1000 Val loss: 3.8601633605139796e-06\n","510 / 1000 Val loss: 7.444817583746044e-06\n","511 / 1000 Val loss: 3.2881921470107045e-06\n","512 / 1000 Val loss: 3.468355544100632e-06\n","513 / 1000 Val loss: 4.12398912885692e-06\n","514 / 1000 Val loss: 4.3899312913708854e-06\n","515 / 1000 Val loss: 3.846664185402915e-06\n","516 / 1000 Val loss: 5.561560556088807e-06\n","517 / 1000 Val loss: 2.8971029678359628e-05\n","518 / 1000 Val loss: 3.58199986294494e-06\n","519 / 1000 Val loss: 6.078987098590005e-06\n","520 / 1000 Val loss: 5.238071935309563e-06\n","521 / 1000 Val loss: 3.3131511827377835e-06\n","522 / 1000 Val loss: 5.286087798594963e-06\n","523 / 1000 Val loss: 1.2177591997897252e-05\n","524 / 1000 Val loss: 1.1106085366918705e-05\n","525 / 1000 Val loss: 1.268035157409031e-05\n","526 / 1000 Val loss: 8.21777393866796e-06\n","527 / 1000 Val loss: 4.4202752178534865e-06\n","528 / 1000 Val loss: 3.2549519346503075e-06\n","529 / 1000 Val loss: 4.593525773088913e-06\n","530 / 1000 Val loss: 6.22307834419189e-06\n","531 / 1000 Val loss: 7.907634426373988e-06\n","532 / 1000 Val loss: 9.714974112284835e-06\n","533 / 1000 Val loss: 1.4710015420860145e-05\n","534 / 1000 Val loss: 1.1783687114075292e-05\n","535 / 1000 Val loss: 1.691035140538588e-05\n","536 / 1000 Val loss: 3.84977556677768e-06\n","537 / 1000 Val loss: 5.551366939471336e-06\n","538 / 1000 Val loss: 4.4506155063572805e-06\n","539 / 1000 Val loss: 3.90443483411218e-06\n","540 / 1000 Val loss: 3.4894453619926935e-06\n","541 / 1000 Val loss: 3.4597519515955355e-06\n","542 / 1000 Val loss: 3.567051862773951e-06\n","543 / 1000 Val loss: 3.8078096622484736e-06\n","544 / 1000 Val loss: 1.4764231309527531e-05\n","545 / 1000 Val loss: 9.699582733446732e-06\n","546 / 1000 Val loss: 8.321039786096662e-06\n","547 / 1000 Val loss: 4.814722615265055e-06\n","548 / 1000 Val loss: 7.034220743662445e-06\n","549 / 1000 Val loss: 7.4831582423939835e-06\n","550 / 1000 Val loss: 8.551385690225288e-06\n","551 / 1000 Val loss: 1.3016357115702704e-05\n","552 / 1000 Val loss: 1.0756766641861759e-05\n","553 / 1000 Val loss: 1.9060735212406144e-05\n","554 / 1000 Val loss: 2.6918796720565297e-05\n","555 / 1000 Val loss: 2.7504764148034155e-05\n","556 / 1000 Val loss: 1.332244664808968e-05\n","557 / 1000 Val loss: 6.296176252362784e-06\n","558 / 1000 Val loss: 1.3916014722781256e-05\n","559 / 1000 Val loss: 1.8411825294606388e-05\n","560 / 1000 Val loss: 8.060736035986338e-06\n","561 / 1000 Val loss: 5.207411049923394e-06\n","562 / 1000 Val loss: 5.1455103857733775e-06\n","563 / 1000 Val loss: 4.636736775864847e-06\n","564 / 1000 Val loss: 4.3508744056453e-06\n","565 / 1000 Val loss: 4.126619387534447e-06\n","566 / 1000 Val loss: 3.7536938179982826e-06\n","567 / 1000 Val loss: 3.2368423035222804e-06\n","568 / 1000 Val loss: 2.986974322993774e-06\n","569 / 1000 Val loss: 2.8367694540065713e-06\n","570 / 1000 Val loss: 2.8175868465041276e-06\n","571 / 1000 Val loss: 3.141632078040857e-06\n","572 / 1000 Val loss: 7.064369128784165e-05\n","573 / 1000 Val loss: 7.492357781302417e-06\n","574 / 1000 Val loss: 8.491595508530736e-06\n","575 / 1000 Val loss: 1.0610710887704045e-05\n","576 / 1000 Val loss: 1.2234643691044766e-05\n","577 / 1000 Val loss: 2.4621165721327998e-05\n","578 / 1000 Val loss: 5.101205260871211e-06\n","579 / 1000 Val loss: 3.3585304208827438e-06\n","580 / 1000 Val loss: 3.416892468521837e-06\n","581 / 1000 Val loss: 3.934456799470354e-06\n","582 / 1000 Val loss: 4.19106254412327e-06\n","583 / 1000 Val loss: 4.267917574907187e-06\n","584 / 1000 Val loss: 4.586186150845606e-06\n","585 / 1000 Val loss: 4.812831775780069e-06\n","586 / 1000 Val loss: 4.647014975489583e-06\n","587 / 1000 Val loss: 4.174650712229777e-06\n","588 / 1000 Val loss: 3.737583483598428e-06\n","589 / 1000 Val loss: 3.9544088394904975e-06\n","590 / 1000 Val loss: 3.951951384806307e-06\n","591 / 1000 Val loss: 4.196133431833005e-06\n","592 / 1000 Val loss: 1.4949654541851487e-05\n","593 / 1000 Val loss: 4.367683231976116e-06\n","594 / 1000 Val loss: 4.764342065755045e-06\n","595 / 1000 Val loss: 8.652674296172336e-06\n","596 / 1000 Val loss: 5.469461484608473e-06\n","597 / 1000 Val loss: 9.38858102017548e-06\n","598 / 1000 Val loss: 6.0115121414128225e-06\n","599 / 1000 Val loss: 4.156247541686753e-06\n","600 / 1000 Val loss: 3.5739860777539434e-06\n","601 / 1000 Val loss: 3.552130465322989e-06\n","602 / 1000 Val loss: 3.5811567613563966e-06\n","603 / 1000 Val loss: 5.391643753682729e-06\n","604 / 1000 Val loss: 2.0140132619417273e-05\n","605 / 1000 Val loss: 7.806515895936172e-06\n","606 / 1000 Val loss: 1.1743172763090115e-05\n","607 / 1000 Val loss: 9.593079084879719e-06\n","608 / 1000 Val loss: 5.704378963855561e-06\n","609 / 1000 Val loss: 5.8871673900284804e-06\n","610 / 1000 Val loss: 5.311374934535706e-06\n","611 / 1000 Val loss: 1.926906406879425e-05\n","612 / 1000 Val loss: 1.3332165508472826e-05\n","613 / 1000 Val loss: 1.4046591786609497e-05\n","614 / 1000 Val loss: 1.643768155190628e-05\n","615 / 1000 Val loss: 2.617766585899517e-05\n","616 / 1000 Val loss: 8.174984031938948e-06\n","617 / 1000 Val loss: 1.5904954125289805e-05\n","618 / 1000 Val loss: 1.4234668924473226e-05\n","619 / 1000 Val loss: 1.3032940842094831e-05\n","620 / 1000 Val loss: 1.348275600321358e-05\n","621 / 1000 Val loss: 1.7042146282619797e-05\n","622 / 1000 Val loss: 2.7019630579161458e-05\n","623 / 1000 Val loss: 2.6974323191097938e-05\n","624 / 1000 Val loss: 1.5930132576613687e-05\n","625 / 1000 Val loss: 5.8294262998970225e-06\n","626 / 1000 Val loss: 3.7391578189271968e-06\n","627 / 1000 Val loss: 3.928513251594268e-06\n","628 / 1000 Val loss: 4.1486268855805974e-06\n","629 / 1000 Val loss: 4.21332651967532e-06\n","630 / 1000 Val loss: 4.412696853250964e-06\n","631 / 1000 Val loss: 4.8845399760466535e-06\n","632 / 1000 Val loss: 5.891675300517818e-06\n","633 / 1000 Val loss: 7.714521416346543e-06\n","634 / 1000 Val loss: 9.71760255197296e-06\n","635 / 1000 Val loss: 1.1740645277313888e-05\n","636 / 1000 Val loss: 1.4382807421498e-05\n","637 / 1000 Val loss: 3.984550858149305e-06\n","638 / 1000 Val loss: 5.400905138230883e-06\n","639 / 1000 Val loss: 6.446216048061615e-06\n","640 / 1000 Val loss: 8.495113434037194e-06\n","641 / 1000 Val loss: 6.0061533986299764e-06\n","642 / 1000 Val loss: 5.292361947795143e-06\n","643 / 1000 Val loss: 4.0426061787002254e-06\n","644 / 1000 Val loss: 3.207062945875805e-06\n","645 / 1000 Val loss: 2.8959923383808928e-06\n","646 / 1000 Val loss: 2.701252697079326e-06\n","647 / 1000 Val loss: 2.6021223220595857e-06\n","648 / 1000 Val loss: 4.82027598991408e-06\n","649 / 1000 Val loss: 1.3452151506498922e-05\n","650 / 1000 Val loss: 3.198234935553046e-06\n","651 / 1000 Val loss: 3.0179858185874764e-06\n","652 / 1000 Val loss: 8.627282113593537e-06\n","653 / 1000 Val loss: 7.840269972803071e-06\n","654 / 1000 Val loss: 6.383075742633082e-06\n","655 / 1000 Val loss: 4.589295258483617e-06\n","656 / 1000 Val loss: 4.437174538907129e-06\n","657 / 1000 Val loss: 4.065767825522926e-06\n","658 / 1000 Val loss: 3.873050445690751e-06\n","659 / 1000 Val loss: 3.388284994798596e-06\n","660 / 1000 Val loss: 3.463802840997232e-06\n","661 / 1000 Val loss: 3.1598444820701843e-06\n","662 / 1000 Val loss: 2.9548264137702063e-06\n","663 / 1000 Val loss: 2.9304599138413323e-06\n","664 / 1000 Val loss: 1.9257689928053878e-05\n","665 / 1000 Val loss: 2.17874094232684e-05\n","666 / 1000 Val loss: 3.524436579027679e-06\n","667 / 1000 Val loss: 9.861858416115865e-06\n","668 / 1000 Val loss: 4.515309228736442e-06\n","669 / 1000 Val loss: 5.244681233307347e-06\n","670 / 1000 Val loss: 4.600947704602731e-06\n","671 / 1000 Val loss: 3.0744959076400846e-06\n","672 / 1000 Val loss: 3.7524446270253975e-06\n","673 / 1000 Val loss: 1.1522284694365226e-05\n","674 / 1000 Val loss: 7.4289146141381934e-06\n","675 / 1000 Val loss: 7.061677479214268e-06\n","676 / 1000 Val loss: 1.0870405276364181e-05\n","677 / 1000 Val loss: 7.6641754276352e-06\n","678 / 1000 Val loss: 4.611728400050197e-06\n","679 / 1000 Val loss: 7.547749646619195e-06\n","680 / 1000 Val loss: 4.4757303840015084e-05\n","681 / 1000 Val loss: 1.4554905646946281e-05\n","682 / 1000 Val loss: 1.0215399925073143e-05\n","683 / 1000 Val loss: 4.862151854467811e-06\n","684 / 1000 Val loss: 5.088847956358222e-06\n","685 / 1000 Val loss: 6.103554824221646e-06\n","686 / 1000 Val loss: 6.7206819949205965e-06\n","687 / 1000 Val loss: 7.305684903258225e-06\n","688 / 1000 Val loss: 8.211881322495174e-06\n","689 / 1000 Val loss: 9.131426850217395e-06\n","690 / 1000 Val loss: 9.435413630853873e-06\n","691 / 1000 Val loss: 7.90405465522781e-06\n","692 / 1000 Val loss: 4.804579020856181e-06\n","693 / 1000 Val loss: 9.029456123244017e-06\n","694 / 1000 Val loss: 2.132229201379232e-05\n","695 / 1000 Val loss: 9.261746527045034e-06\n","696 / 1000 Val loss: 3.605279744078871e-06\n","697 / 1000 Val loss: 1.7559681509737857e-05\n","698 / 1000 Val loss: 1.1673936569422949e-05\n","699 / 1000 Val loss: 9.37825461733155e-06\n","700 / 1000 Val loss: 7.109948455763515e-06\n","701 / 1000 Val loss: 5.071183295513038e-06\n","702 / 1000 Val loss: 4.428413831192302e-06\n","703 / 1000 Val loss: 4.0990407796925865e-06\n","704 / 1000 Val loss: 4.189343599136919e-06\n","705 / 1000 Val loss: 4.433367848832859e-06\n","706 / 1000 Val loss: 4.246811840857845e-06\n","707 / 1000 Val loss: 4.3342010940250475e-06\n","708 / 1000 Val loss: 3.989399829151807e-06\n","709 / 1000 Val loss: 4.014232672489015e-06\n","710 / 1000 Val loss: 3.4918625715363305e-06\n","711 / 1000 Val loss: 4.4891426114190836e-06\n","712 / 1000 Val loss: 4.7390396503033116e-05\n","713 / 1000 Val loss: 1.1058647032768931e-05\n","714 / 1000 Val loss: 1.4146001376502682e-05\n","715 / 1000 Val loss: 1.1697115951392334e-05\n","716 / 1000 Val loss: 5.943916221440304e-06\n","717 / 1000 Val loss: 5.017497642256785e-06\n","718 / 1000 Val loss: 3.7181507650529966e-06\n","719 / 1000 Val loss: 3.697517513501225e-06\n","720 / 1000 Val loss: 4.157504008617252e-06\n","721 / 1000 Val loss: 4.6411500989052e-06\n","722 / 1000 Val loss: 4.793233074451564e-06\n","723 / 1000 Val loss: 4.762647222378291e-06\n","724 / 1000 Val loss: 4.461233402253129e-06\n","725 / 1000 Val loss: 4.088950845471118e-06\n","726 / 1000 Val loss: 3.6747480862686643e-06\n","727 / 1000 Val loss: 3.222470240871189e-06\n","728 / 1000 Val loss: 3.015820311702555e-06\n","729 / 1000 Val loss: 2.954254341602791e-06\n","730 / 1000 Val loss: 3.013647301486344e-06\n","731 / 1000 Val loss: 2.951791884697741e-06\n","732 / 1000 Val loss: 3.353776719450252e-06\n","733 / 1000 Val loss: 7.415426807710901e-06\n","734 / 1000 Val loss: 4.8392034841526765e-06\n","735 / 1000 Val loss: 3.623025577326189e-06\n","736 / 1000 Val loss: 3.180387466272805e-06\n","737 / 1000 Val loss: 3.1230345030053286e-06\n","738 / 1000 Val loss: 1.0540416951698717e-05\n","739 / 1000 Val loss: 1.291194621444447e-05\n","740 / 1000 Val loss: 3.6627884583140258e-06\n","741 / 1000 Val loss: 5.525467713596299e-06\n","742 / 1000 Val loss: 5.333489298209315e-06\n","743 / 1000 Val loss: 1.246926149178762e-05\n","744 / 1000 Val loss: 8.070833246165421e-06\n","745 / 1000 Val loss: 8.500554940837901e-06\n","746 / 1000 Val loss: 8.110229828162119e-06\n","747 / 1000 Val loss: 8.83268967299955e-06\n","748 / 1000 Val loss: 1.012974917102838e-05\n","749 / 1000 Val loss: 1.1672746040858328e-05\n","750 / 1000 Val loss: 1.1663337318168487e-05\n","751 / 1000 Val loss: 6.7045202740700915e-06\n","752 / 1000 Val loss: 2.3789536498952657e-05\n","753 / 1000 Val loss: 5.341733867680887e-06\n","754 / 1000 Val loss: 2.64147347479593e-05\n","755 / 1000 Val loss: 1.7330723494524136e-05\n","756 / 1000 Val loss: 1.6815087292343378e-05\n","757 / 1000 Val loss: 3.614081606428954e-06\n","758 / 1000 Val loss: 3.1050526558829006e-06\n","759 / 1000 Val loss: 3.4789209166774526e-06\n","760 / 1000 Val loss: 3.734460960913566e-06\n","761 / 1000 Val loss: 4.251115115039283e-06\n","762 / 1000 Val loss: 4.569883458316326e-06\n","763 / 1000 Val loss: 4.937347966915695e-06\n","764 / 1000 Val loss: 5.45658394912607e-06\n","765 / 1000 Val loss: 6.549017143697711e-06\n","766 / 1000 Val loss: 7.034604095679242e-06\n","767 / 1000 Val loss: 9.257196325052064e-06\n","768 / 1000 Val loss: 1.8295922927791253e-05\n","769 / 1000 Val loss: 1.2702364983852021e-05\n","770 / 1000 Val loss: 5.833539489685791e-06\n","771 / 1000 Val loss: 3.974600986111909e-06\n","772 / 1000 Val loss: 3.861431650875602e-06\n","773 / 1000 Val loss: 3.881120392179582e-06\n","774 / 1000 Val loss: 3.6565811569744255e-06\n","775 / 1000 Val loss: 3.4124054764106404e-06\n","776 / 1000 Val loss: 3.0241781132644974e-06\n","777 / 1000 Val loss: 2.7640153348329477e-06\n","778 / 1000 Val loss: 2.701477342270664e-06\n","779 / 1000 Val loss: 3.113610546279233e-06\n","780 / 1000 Val loss: 5.520793820323888e-06\n","781 / 1000 Val loss: 9.244567081623245e-06\n","782 / 1000 Val loss: 1.500385315011954e-05\n","783 / 1000 Val loss: 1.1803055713244248e-05\n","784 / 1000 Val loss: 6.212675998540362e-06\n","785 / 1000 Val loss: 2.6148263714276254e-06\n","786 / 1000 Val loss: 7.999149602255784e-06\n","787 / 1000 Val loss: 1.7912896510097198e-05\n","788 / 1000 Val loss: 2.0489243979682215e-05\n","789 / 1000 Val loss: 3.286717628725455e-06\n","790 / 1000 Val loss: 3.331982270537992e-06\n","791 / 1000 Val loss: 3.9559108699904755e-06\n","792 / 1000 Val loss: 4.216897650621831e-06\n","793 / 1000 Val loss: 3.876206847053254e-06\n","794 / 1000 Val loss: 3.7245497424009955e-06\n","795 / 1000 Val loss: 3.473749075055821e-06\n","796 / 1000 Val loss: 3.6715850910695735e-06\n","797 / 1000 Val loss: 3.616103867898346e-06\n","798 / 1000 Val loss: 3.481952262518462e-06\n","799 / 1000 Val loss: 3.4192817111033946e-06\n","800 / 1000 Val loss: 3.5990112792205764e-06\n","801 / 1000 Val loss: 3.302823870399152e-06\n","802 / 1000 Val loss: 3.211175226169871e-06\n","803 / 1000 Val loss: 3.239064881199738e-06\n","804 / 1000 Val loss: 3.556147248673369e-06\n","805 / 1000 Val loss: 3.412273372305208e-06\n","806 / 1000 Val loss: 3.7339711980166612e-06\n","807 / 1000 Val loss: 4.062838797835866e-06\n","808 / 1000 Val loss: 6.431560450437246e-06\n","809 / 1000 Val loss: 1.003891975415172e-05\n","810 / 1000 Val loss: 5.951961611572187e-06\n","811 / 1000 Val loss: 3.674145091281389e-06\n","812 / 1000 Val loss: 5.930206043558428e-06\n","813 / 1000 Val loss: 3.5957682484877296e-06\n","814 / 1000 Val loss: 2.7124690404889407e-06\n","815 / 1000 Val loss: 2.9083573735988466e-06\n","816 / 1000 Val loss: 2.820340341713745e-06\n","817 / 1000 Val loss: 6.377284080372192e-06\n","818 / 1000 Val loss: 7.646178346476518e-06\n","819 / 1000 Val loss: 1.4384991118276957e-05\n","820 / 1000 Val loss: 5.053158474765951e-06\n","821 / 1000 Val loss: 9.626933206163812e-06\n","822 / 1000 Val loss: 8.003277798707131e-06\n","823 / 1000 Val loss: 8.893455742509104e-06\n","824 / 1000 Val loss: 7.481114153051749e-06\n","825 / 1000 Val loss: 7.163717782532331e-06\n","826 / 1000 Val loss: 1.330442501057405e-05\n","827 / 1000 Val loss: 4.693516984843882e-06\n","828 / 1000 Val loss: 9.71487406786764e-06\n","829 / 1000 Val loss: 8.906812581699342e-06\n","830 / 1000 Val loss: 5.013348527427297e-06\n","831 / 1000 Val loss: 3.555865077942144e-06\n","832 / 1000 Val loss: 3.6527153497445397e-06\n","833 / 1000 Val loss: 3.492467158139334e-06\n","834 / 1000 Val loss: 2.967004093079595e-06\n","835 / 1000 Val loss: 2.7187529667571653e-06\n","836 / 1000 Val loss: 2.683002094272524e-06\n","837 / 1000 Val loss: 2.9065522539895028e-06\n","838 / 1000 Val loss: 4.726374754682183e-06\n","839 / 1000 Val loss: 7.670657396374736e-06\n","840 / 1000 Val loss: 6.60701016386156e-06\n","841 / 1000 Val loss: 3.354542059241794e-06\n","842 / 1000 Val loss: 7.508794624300208e-06\n","843 / 1000 Val loss: 5.438864718598779e-06\n","844 / 1000 Val loss: 5.576508556259796e-06\n","845 / 1000 Val loss: 4.488350441533839e-06\n","846 / 1000 Val loss: 3.5783107250608737e-06\n","847 / 1000 Val loss: 6.2428116507362574e-06\n","848 / 1000 Val loss: 1.826989682740532e-05\n","849 / 1000 Val loss: 1.1522733984747902e-05\n","850 / 1000 Val loss: 3.2042684324551374e-05\n","851 / 1000 Val loss: 1.2082976354577113e-05\n","852 / 1000 Val loss: 6.403354745998513e-06\n","853 / 1000 Val loss: 3.245219704695046e-05\n","854 / 1000 Val loss: 3.4255385799042415e-06\n","855 / 1000 Val loss: 5.24007646163227e-06\n","856 / 1000 Val loss: 7.0512487582163885e-06\n","857 / 1000 Val loss: 5.576981038757367e-06\n","858 / 1000 Val loss: 5.180566859053215e-06\n","859 / 1000 Val loss: 4.7084381549211685e-06\n","860 / 1000 Val loss: 4.644948603527155e-06\n","861 / 1000 Val loss: 4.779798018716974e-06\n","862 / 1000 Val loss: 5.022918685426703e-06\n","863 / 1000 Val loss: 5.8607320170267485e-06\n","864 / 1000 Val loss: 7.5052957981824875e-06\n","865 / 1000 Val loss: 1.2287096069485415e-05\n","866 / 1000 Val loss: 1.2813859029847663e-05\n","867 / 1000 Val loss: 3.4947061067214236e-05\n","868 / 1000 Val loss: 3.735713107744232e-05\n","869 / 1000 Val loss: 5.012120254832553e-06\n","870 / 1000 Val loss: 6.023368769092485e-06\n","871 / 1000 Val loss: 1.3627819498651661e-05\n","872 / 1000 Val loss: 9.856383258011192e-06\n","873 / 1000 Val loss: 4.340872692409903e-06\n","874 / 1000 Val loss: 6.520272563648177e-06\n","875 / 1000 Val loss: 1.0274221494910307e-05\n","876 / 1000 Val loss: 1.3181957001506817e-05\n","877 / 1000 Val loss: 1.341144979960518e-05\n","878 / 1000 Val loss: 1.1263773558312096e-05\n","879 / 1000 Val loss: 9.04460102901794e-06\n","880 / 1000 Val loss: 7.662842108402401e-06\n","881 / 1000 Val loss: 6.6655647970037535e-06\n","882 / 1000 Val loss: 5.945489647274371e-06\n","883 / 1000 Val loss: 5.4079227993497625e-06\n","884 / 1000 Val loss: 4.857229669141816e-06\n","885 / 1000 Val loss: 4.664931566367159e-06\n","886 / 1000 Val loss: 5.373218755266862e-06\n","887 / 1000 Val loss: 7.226691650430439e-06\n","888 / 1000 Val loss: 5.193983724893769e-06\n","889 / 1000 Val loss: 5.888948180654552e-06\n","890 / 1000 Val loss: 8.790701031102799e-06\n","891 / 1000 Val loss: 4.614461886376375e-06\n","892 / 1000 Val loss: 5.192418484512018e-06\n","893 / 1000 Val loss: 8.594151950092055e-06\n","894 / 1000 Val loss: 2.9135778731870232e-06\n","895 / 1000 Val loss: 2.888625431296532e-06\n","896 / 1000 Val loss: 3.5106570521747926e-06\n","897 / 1000 Val loss: 4.66373194285552e-06\n","898 / 1000 Val loss: 5.9336930462450255e-06\n","899 / 1000 Val loss: 3.7291767966962652e-06\n","900 / 1000 Val loss: 7.36877518647816e-06\n","901 / 1000 Val loss: 3.5515511171979597e-06\n","902 / 1000 Val loss: 2.640878847159911e-06\n","903 / 1000 Val loss: 2.907510179284145e-06\n","904 / 1000 Val loss: 3.1791669243830256e-06\n","905 / 1000 Val loss: 3.2958500923996326e-06\n","906 / 1000 Val loss: 3.303138782939641e-06\n","907 / 1000 Val loss: 3.2979385196085786e-06\n","908 / 1000 Val loss: 3.498660362311057e-06\n","909 / 1000 Val loss: 3.471780246400158e-06\n","910 / 1000 Val loss: 3.317863274787669e-06\n","911 / 1000 Val loss: 3.0502292247547302e-06\n","912 / 1000 Val loss: 3.631389063230017e-06\n","913 / 1000 Val loss: 2.4440652850898914e-05\n","914 / 1000 Val loss: 2.411387686152011e-05\n","915 / 1000 Val loss: 1.2026491276628803e-05\n","916 / 1000 Val loss: 1.5938276192173362e-05\n","917 / 1000 Val loss: 1.2745210369757842e-05\n","918 / 1000 Val loss: 1.2280961527721956e-05\n","919 / 1000 Val loss: 1.343966414424358e-05\n","920 / 1000 Val loss: 8.221229109039996e-06\n","921 / 1000 Val loss: 5.034033165429719e-06\n","922 / 1000 Val loss: 5.2013842832820956e-06\n","923 / 1000 Val loss: 5.563087142945733e-06\n","924 / 1000 Val loss: 6.765300440747524e-06\n","925 / 1000 Val loss: 8.185303158825263e-06\n","926 / 1000 Val loss: 8.518856702721678e-06\n","927 / 1000 Val loss: 7.6187243394088e-06\n","928 / 1000 Val loss: 6.727512754878262e-06\n","929 / 1000 Val loss: 5.911777861911105e-06\n","930 / 1000 Val loss: 5.646350473398343e-06\n","931 / 1000 Val loss: 5.526707809622167e-06\n","932 / 1000 Val loss: 5.9162416619074065e-06\n","933 / 1000 Val loss: 6.632604254264152e-06\n","934 / 1000 Val loss: 7.830282811482903e-06\n","935 / 1000 Val loss: 1.0649774594639894e-05\n","936 / 1000 Val loss: 1.8122707842849195e-05\n","937 / 1000 Val loss: 4.555960185825825e-05\n","938 / 1000 Val loss: 9.926840903062839e-06\n","939 / 1000 Val loss: 1.7854543330031447e-05\n","940 / 1000 Val loss: 1.3562494132202119e-05\n","941 / 1000 Val loss: 5.801896804769058e-06\n","942 / 1000 Val loss: 6.5399899540352635e-06\n","943 / 1000 Val loss: 5.7892011682270095e-06\n","944 / 1000 Val loss: 4.896074642601889e-06\n","945 / 1000 Val loss: 4.9826739996206015e-06\n","946 / 1000 Val loss: 4.829368663195055e-06\n","947 / 1000 Val loss: 4.67666450276738e-06\n","948 / 1000 Val loss: 4.625530436896952e-06\n","949 / 1000 Val loss: 4.756312591780443e-06\n","950 / 1000 Val loss: 5.049634182796581e-06\n","951 / 1000 Val loss: 5.3849003052164335e-06\n","952 / 1000 Val loss: 5.659898306475952e-06\n","953 / 1000 Val loss: 5.889320618734928e-06\n","954 / 1000 Val loss: 6.200913503562333e-06\n","955 / 1000 Val loss: 6.7013520492764656e-06\n","956 / 1000 Val loss: 7.450220437021926e-06\n","957 / 1000 Val loss: 8.490216714562848e-06\n","958 / 1000 Val loss: 9.627498002373613e-06\n","959 / 1000 Val loss: 1.0532526175666135e-05\n","960 / 1000 Val loss: 1.0593689694360364e-05\n","961 / 1000 Val loss: 9.79987089522183e-06\n","962 / 1000 Val loss: 9.963660886569414e-06\n","963 / 1000 Val loss: 1.0651036063791253e-05\n","964 / 1000 Val loss: 1.0878368811972905e-05\n","965 / 1000 Val loss: 5.073858574178303e-06\n","966 / 1000 Val loss: 9.402921023138333e-06\n","967 / 1000 Val loss: 5.978434273856692e-06\n","968 / 1000 Val loss: 3.5703926641872386e-06\n","969 / 1000 Val loss: 2.636030103531084e-06\n","970 / 1000 Val loss: 2.9688656013604486e-06\n","971 / 1000 Val loss: 4.643669399229111e-06\n","972 / 1000 Val loss: 6.0903930716449395e-06\n","973 / 1000 Val loss: 1.225309733854374e-05\n","974 / 1000 Val loss: 8.01441910880385e-06\n","975 / 1000 Val loss: 3.086336391788791e-06\n","976 / 1000 Val loss: 4.726095539808739e-06\n","977 / 1000 Val loss: 3.898629529430764e-06\n","978 / 1000 Val loss: 3.984453542216215e-06\n","979 / 1000 Val loss: 3.828280568995979e-06\n","980 / 1000 Val loss: 4.228043053444708e-06\n","981 / 1000 Val loss: 3.4963572943524923e-06\n","982 / 1000 Val loss: 3.4613647130754543e-06\n","983 / 1000 Val loss: 3.6845544855168555e-06\n","984 / 1000 Val loss: 7.345949143200414e-06\n","985 / 1000 Val loss: 5.754787707701325e-06\n","986 / 1000 Val loss: 5.85873203817755e-06\n","987 / 1000 Val loss: 5.616781436401652e-06\n","988 / 1000 Val loss: 3.7751506170025095e-06\n","989 / 1000 Val loss: 3.6074966374144424e-06\n","990 / 1000 Val loss: 3.873119112540735e-06\n","991 / 1000 Val loss: 4.255862677382538e-06\n","992 / 1000 Val loss: 6.786451649531955e-06\n","993 / 1000 Val loss: 1.3985642908664886e-05\n","994 / 1000 Val loss: 6.519176622532541e-06\n","995 / 1000 Val loss: 1.4739121979800984e-05\n","996 / 1000 Val loss: 4.640882252715528e-06\n","997 / 1000 Val loss: 7.661778909096029e-06\n","998 / 1000 Val loss: 5.23833705301513e-06\n","999 / 1000 Val loss: 3.752471002371749e-06\n"]}]},{"cell_type":"markdown","source":["Save weights and biases"],"metadata":{"id":"KvxUlX6EKqdX"}},{"cell_type":"code","source":["w1 = rnn.i2l1.weight.cpu().detach().numpy()\n","w2 = rnn.l1l2.weight.cpu().detach().numpy()\n","w3 = rnn.l2l3.weight.cpu().detach().numpy()\n","w_output = rnn.m2o.weight.cpu().detach().numpy()\n","b1 = rnn.i2l1.bias.cpu().detach().numpy()\n","b2 = rnn.l1l2.bias.cpu().detach().numpy()\n","b3 = rnn.l2l3.bias.cpu().detach().numpy()\n","b_output = rnn.m2o.bias.cpu().detach().numpy()\n","\n","\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w1_1layer.txt', w1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w2_1layer.txt', w2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w3_1layer.txt', w3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/wo_1layer.txt', w_output)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b1_1layer.txt', b1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b2_1layer.txt', b2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b3_1layer.txt', b3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/bo_1layer.txt', b_output)"],"metadata":{"id":"lPTHjJnOKr6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Good model? Save it"],"metadata":{"id":"VvBXsv-0e8z4"}},{"cell_type":"code","source":["torch.save(rnn.state_dict(), '/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/rnn_1.6.pth')"],"metadata":{"id":"pfEDra0MBM2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot predicted R vs actual R"],"metadata":{"id":"tbR_hwv_fhGj"}},{"cell_type":"code","source":["output1, loss1 = test(rnn, input_tensor_1l_test, answer_tensor_1l_test, criterion, optimizer, batch_size_test, device)\n","\n","output = output1.to('cpu').detach().numpy()\n","answer = answer_tensor_1l_test.to('cpu').detach().numpy()\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output1_plainNN.txt', output)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer1_plainNN.txt', answer)\n","\n","\n","plt.plot(answer[:, 0], output[:, 0], \"bo\")\n","plt.ylabel('Predicted R')\n","plt.xlabel('True R')\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"Nj5YH08EfXTA","executionInfo":{"status":"ok","timestamp":1719839978075,"user_tz":240,"elapsed":498,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"0c73f721-fd97-4ebb-c916-d0cc12f24acb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Load and run saved RNN"],"metadata":{"id":"cPHo_JiE45J8"}},{"cell_type":"code","source":["np.random.seed(128)\n","torch.manual_seed(128)\n","input_dim = 4\n","# hyperparameters to tune\n","hidden_layer = 1024\n","hidden_layer2 = 1024\n","hidden_size = 16\n","# output is R, A, T\n","output_size = 3\n","batch_size = 585\n","batch_size_test = 1000\n","num_batches = int(18720/batch_size)\n","epochs = 1000\n","\n","rnn = RNN(input_dim, hidden_layer, hidden_layer2, hidden_size, output_size)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Training on: \" + str(device))\n","rnn = rnn.to(device)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n","current_loss_train = 0\n","current_loss_test = 0\n","loss_plot_test = np.zeros(epochs)\n","loss_plot_train = np.zeros(epochs*num_batches)\n","x_axis = np.linspace(0, epochs, num=epochs*num_batches)\n","\n","\n","state_dict = torch.load('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/rnn_1.6.pth')\n","\n","rnn.load_state_dict(state_dict)\n","\n","\n","output1, loss1 = test(rnn, input_tensor_1l_test, answer_tensor_1l_test, criterion, optimizer, batch_size_test, device)\n","output2, loss2 = test(rnn, input_tensor_2l_test, answer_tensor_2l_test, criterion, optimizer, batch_size_test, device)\n","output3, loss3 = test(rnn, input_tensor_3l_test, answer_tensor_3l_test, criterion, optimizer, batch_size_test, device)\n","output4, loss4 = test(rnn, input_tensor_4l_test, answer_tensor_4l_test, criterion, optimizer, batch_size_test, device)\n","output5, loss5 = test(rnn, input_tensor_5l_test, answer_tensor_5l_test, criterion, optimizer, batch_size_test, device)\n","output6, loss6 = test(rnn, input_tensor_6l_test, answer_tensor_6l_test, criterion, optimizer, batch_size_test, device)\n","print(loss1, loss2, loss3, loss4, loss5, loss6)\n","output1 = output1.to('cpu').detach().numpy()\n","output2 = output2.to('cpu').detach().numpy()\n","output3 = output3.to('cpu').detach().numpy()\n","output4 = output4.to('cpu').detach().numpy()\n","output5 = output5.to('cpu').detach().numpy()\n","output6 = output6.to('cpu').detach().numpy()\n","answer1 = answer_tensor_1l_test.to('cpu').detach().numpy()\n","answer2 = answer_tensor_2l_test.to('cpu').detach().numpy()\n","answer3 = answer_tensor_3l_test.to('cpu').detach().numpy()\n","answer4 = answer_tensor_4l_test.to('cpu').detach().numpy()\n","answer5 = answer_tensor_5l_test.to('cpu').detach().numpy()\n","answer6 = answer_tensor_6l_test.to('cpu').detach().numpy()\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output1_RNN.txt', output1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output2_RNN.txt', output2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output3_RNN.txt', output3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output4_RNN.txt', output4)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output5_RNN.txt', output5)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/output6_RNN.txt', output6)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer1_RNN.txt', answer1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer2_RNN.txt', answer2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer3_RNN.txt', answer3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer4_RNN.txt', answer4)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer5_RNN.txt', answer5)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/answer6_RNN.txt', answer6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-UJvrMrf44a1","executionInfo":{"status":"ok","timestamp":1719789388266,"user_tz":240,"elapsed":9331,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"211738f9-cc8a-450e-9595-f12375169240"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on: cuda\n","3.167288923577871e-06 1.342157611361472e-05 1.3006816516281106e-05 9.957088877854403e-06 1.8602457203087397e-05 1.6610361853963695e-05\n"]}]},{"cell_type":"markdown","source":["Save weights and biases to numpy array"],"metadata":{"id":"kgPhkLVGJ8dF"}},{"cell_type":"code","source":["np.random.seed(128)\n","torch.manual_seed(128)\n","input_dim = 4\n","# hyperparameters to tune\n","hidden_layer = 1024\n","hidden_layer2 = 1024\n","hidden_size = 16\n","# output is R, A, T\n","output_size = 3\n","batch_size = 585\n","batch_size_test = 1000\n","num_batches = int(18720/batch_size)\n","epochs = 1000\n","state_dict = torch.load('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/rnn_1.6.pth')\n","rnn = RNN(input_dim, hidden_layer, hidden_layer2, hidden_size, output_size)\n","rnn.load_state_dict(state_dict)\n","\n","w1 = rnn.i2l1.weight.cpu().detach().numpy()\n","w2 = rnn.l1l2.weight.cpu().detach().numpy()\n","w3 = rnn.l2l3.weight.cpu().detach().numpy()\n","w_output = rnn.m2o.weight.cpu().detach().numpy()\n","w_hidden = rnn.m2h.weight.cpu().detach().numpy()\n","b1 = rnn.i2l1.bias.cpu().detach().numpy()\n","b2 = rnn.l1l2.bias.cpu().detach().numpy()\n","b3 = rnn.l2l3.bias.cpu().detach().numpy()\n","b_output = rnn.m2o.bias.cpu().detach().numpy()\n","b_hidden = rnn.m2h.bias.cpu().detach().numpy()\n","print(w_hidden.shape)\n","\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w1_rnn1.6.txt', w1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w2_rnn1.6.txt', w2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/w3_rnn1.6.txt', w3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/wo_rnn1.6.txt', w_output)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/wh_rnn1.6.txt', w_hidden)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b1_rnn1.6.txt', b1)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b2_rnn1.6.txt', b2)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/b3_rnn1.6.txt', b3)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/bo_rnn1.6.txt', b_output)\n","np.savetxt('/content/drive/My Drive/Colab Notebooks/RNN_Monte_Carlo/bh_rnn1.6.txt', b_hidden)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvX8kROHJ8ML","executionInfo":{"status":"ok","timestamp":1719771284113,"user_tz":240,"elapsed":12703,"user":{"displayName":"Danny Carne","userId":"01450206690087310829"}},"outputId":"46249848-5d84-46bb-fdf1-405ab66ce648"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(16, 1024)\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1HJteKqu5YncUhrzyUiyS1Eig7Zvym9-B","timestamp":1735755723869},{"file_id":"17Q8-I_0266Fw2MI13OAynylXeCjQi7xw","timestamp":1719261738400}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"38b65d22d35179c6935a22cd0c80ce3ebfa39e2ed4f084b887887c7215100d32"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}